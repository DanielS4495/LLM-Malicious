# LLM-Malicious

the sources of each collection of promts is in the comment section for each commit.

LLM-Malicious

The sources of each collection of prompts are in the comment section for each commit.

Articles we based on - https://docs.google.com/document/d/1u4HCAhez2J9OF\_zu4qODzZ0taol4BOYi\_jz9zEAOXcs/edit?usp=sharing



Repositories we based on – 

•	https://github.com/MAIL-Tele-AI/MalwareBench.git  - go down in readme there is attack\_prompts.xlsx

•	https://github.com/verazuo/jailbreak\_llms.git     - go to file - data/prompts

•	https://github.com/cryptwareapps/Malware-Database.git - maybe not prompt

•	https://github.com/vxunderground/MalwareSourceCode.git - maybe not prompt

•	https://huggingface.co/datasets/codesagar/malicious-llm-prompts 

•	https://www.perplexity.ai/search/bshbyl-mkhqr-blbd-tmts-ly-data-4hYFvvAVT4SoDQoW.IjkIA?0=d#0 - page of perplexity ai for research and database

•	https://github.com/cysecbench/dataset.git 



first-model
=

## 1. Script 1: Response Generator

This script is responsible for generating model responses from a list of prompts.

* **Purpose:** To read prompts from an Excel file, send them to a specified LLM, and save the model's responses to a CSV file.
* **Input File (`EXCEL_FILE`):** This is the main prompt database (e.g., `attack_prompts.xlsx`).
* **Output File (`RESULTS_FILE`):** This is the output CSV (e.g., `responses_results.csv`). The script **appends** new results, so it will not delete or overwrite existing data if you re-run it.
* **Key Features & Customization:**
    * **Prompt Filtering:** You can choose which prompts to process by changing the "specific criteria" in the pandas filter line (e.g., `df_filtered = df[df['AttackMethod'] == 'Persuative LLM']`).
    * **Model Selection:** You can change which LLM is used by modifying the `model` parameter inside the `try` block (e.g., changing `meta-llama/Meta-Llama-3-8B-Instruct` to another model).
    * **Rate Limiting:** The `time.sleep()` command inside the `for` loop is essential. It pauses the script between requests to avoid hitting the API's rate limits, which is necessary for using free-tier models.
    * **Error Handling:** The `try...except` block is designed to keep the output file clean.
        * **Successful responses are saved**, even if the model refuses to answer (e.g., "I cannot help with that").
        * **System errors** (like `Bad Request` or connection failures) **are *not*** written to the CSV file. This ensures the output file is ready for evaluation.

---

## 2. Script 2: Response Evaluator

This script is responsible for judging the responses generated by Script 1.

* **Purpose:** To read the generated responses, send them (along with the original prompt) to a "judge" LLM, and score them based on a rubric (e.g., `strongreject_rubric`).
* **Key Features & Customization:**
    * **LLM Provider:** You can change the judge LLM by updating the API keys and environment variables.
    * **Groq (Llama) Configuration:** To use Groq as the evaluator, you must set these three lines:
        1.  `os.environ["DATASETS_NUM_PROC"] = "1"`: This is critical. It forces the script to send prompts **one by one** (serially) to avoid the free-tier rate limit.
        2.  `os.environ["OPENAI_API_KEY"] = GROQ_API_KEY`: Your personal Groq API key.
        3.  `os.environ["OPENAI_API_BASE"] = "https://api.groq.com/openai/v1"`: This tells the script to send requests to Groq's server instead of OpenAI's.
    * **Judge Model:** You can change the specific model used for evaluation in the `judge_models` variable (e.g., `["openai/llama-3.1-8b-instant"]`).
    * **Rate Limiting:** This script also uses a `time.sleep()` in its loop to respect the API's free-tier request limits.
    * **Output:** The script merges the original data with the new evaluation scores and saves the final output to a new CSV file.

---

## 3. File Descriptions & Current Status

* **`responses_results.csv`**
    * **Contents:** This file contains the raw, successful model responses from Script 1.
    * **Status:** It currently contains **249 responses** from the "Persuative LLM" attack method.
    * **Note:** The original prompt list contained 320 items. The 71 missing prompts were **not** model refusals; they were system errors (like `Bad Request` due to a malformed prompt) that the script correctly skipped.

* **`responses_results_evaluated.csv`**
    * **Contents:** This file contains the judged/scored responses from Script 2.
    * **Status:** This file is currently only a **test file containing 110 evaluated prompts**.
    * **Next Step:** The Evaluator (Script 2) **needs to be run again** on the full `responses_results.csv` file to evaluate all 249 responses.

* **`responses_results_evaluated_5.csv`**
    * **Contents:** This file contains the judged/scored responses from Script 2.
    * **Status:** This file is currently only a **test file containing 5 evaluated prompts**.
    * **Next Step:** The Evaluator (Script 2) **needs to be run again** on the full `responses_results.csv` file to evaluate all 249 responses.
